---
title: "Mixture-of-Experts Implementation"
excerpt: "Implemented a Switch Transformer alongside a conventional autoregressive transformer and trained on TinyShakespeare to research effects of mixture-of-experts architecture on validation loss, sample-efficiency and training time."
collection: portfolio
link: "https://github.com/terru3/moe-kit"
---
