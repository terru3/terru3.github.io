---
title: 'Learning DS / ML Concepts'
date: 2023-12-20
permalink: /posts/2023/12/learning-ds-ml
excerpt_separator: <!--more-->
toc: true
tags:
  - data science
  - machine learning
---

To fully understand a data science or machine learning concept, you need to be exposed to
all three perspectives—the idea, the math, and the code.
<!--more-->

I've skimmed through a large number of papers, Googled my fair share of metrics and formulas, 
inequalities, proofs, but when the concept actually comes up in discussion or in practice, I often find myself missing core chunks
of knowledge to actully implement and tweak it to my use case. I've had to go back multiple times, yes, reading the notes,
but more importantly filling in the gaps of how the underlying math functions and how the code should be structured.

Therefore, I believe to truly grasp concepts, we need to view it from all three angles—what it accomplishes (idea),
why it works (math), and how it is developed in practice (code). Of course, depending on the content, these components will differ in
complexity. I'll walk through a few examples of what this means, which will hopefully illuminate this formulation.

# Self-attention
  ## Why attend to itself?
  ## Q, K, V?
  ## Other attentions (sliding window, block-sparse, attention sinks)
  ## MQA/GQA
  
# Quantization
  ## Model.half()?
  ## Post-Training Quantization (PTQ) vs. Quantization-Aware Training (QAT)

# CANs (Creative Adversarial Networks)
  ## GAN basics
  ## How to be creative?











